class_path: chebai.models.Electra
init_args:
  criterion:
    class_path: torch.nn.BCEWithLogitsLoss
  out_dim: 1037
  optimizer_kwargs:
    lr: 1e-4
  config:
    vocab_size: 1400
    max_position_embeddings: 1800
    num_attention_heads: 8
    num_hidden_layers: 6
    type_vocab_size: 1
  train_metrics: /home/daniel/Documents/Bachelor/BachelorCode/python-chebai/configs/metrics/micro-macro-f1.yml
  val_metrics: /home/daniel/Documents/Bachelor/BachelorCode/python-chebai/configs/metrics/micro-macro-f1.yml
  test_metrics: /home/daniel/Documents/Bachelor/BachelorCode/python-chebai/configs/metrics/micro-macro-f1.yml
