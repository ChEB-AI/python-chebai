# lightning.pytorch==2.0.2
seed_everything: true
model: chebai.models.electra.ElectraPre

trainer:
  min_epochs: 100
model:
  class_path: chebai.models.ElectraPre
  init_args:
    criterion:
      class_path: chebai.models.electra.ElectraPreLoss
    out_dim: null
    optimizer_kwargs:
      lr: 1e-4
    config:
      generator:
        vocab_size: 1400
        max_position_embeddings: 1800
        num_attention_heads: 8
        num_hidden_layers: 6
        type_vocab_size: 1
      discriminator:
        vocab_size: 1400
        max_position_embeddings: 1800
        num_attention_heads: 8
        num_hidden_layers: 6
        type_vocab_size: 1
data:
  class_path: chebai.preprocessing.datasets.pubchem.SWJChem

ckpt_path: null
